from collections import defaultdict
import math
import time
from tqdm import tqdm
import chess
import threading
import config
import numpy as np
from ChessEnv import ChessEnv
from mapper import Mapping
import torch
class MCTS:
    ''' Monte carlo tree search class
        https://gist.github.com/qpwo/c538c6f73727e254fdc7fab81024f6e1
    '''

    def __init__(self, agent, player=False, exploration_weight:int=1, stochastic=False):
        self.Q = defaultdict(int)  # collection of rewards for each node
        self.N = defaultdict(int)  # collection of visits for each node
        self.P = defaultdict(float) # collection of prior probabilty of reaching this node from it's parent
        self.children = dict()     # children of each node
        self.player = player
        self.exploration_weight = exploration_weight
        self.agent = agent
        self.stochastic = stochastic


    def choose(self, node):
        " choose the best successor"

        if node.is_terminal():
            raise RuntimeError(f"choose called on terminal node: {node}")

        if node not in self.children:
            return node.find_random_children()
        
        children = self.children[node]

        sum_of_visits = sum(self.N[child] for child in children)
        probs = [self.N[child]/sum_of_visits for child in children]

        if self.stochastic:
            return np.randm.choice(children, p=probs)
        else:
            return children[np.argmax(probs)]
        
    def get_possible_moves(self, node):
        #  will return all the children nodes that are reachable from node
        return self.children[node]

    def run_simulation(self, n, node):
        
        for _ in tqdm(range(n)):
            self.do_rollout(node)
        

    def do_rollout(self, node):
        "make the tree one layer better"
        path = self._select(node)
        leaf = path[-1]
        reward = self._expand_simulate(leaf)
        self._backpropagate(path, reward)
        
    def map_valid_move(self, move: chess.Move) -> None:
        """
        Input: a valid move generated by the chess library.
        Will add the move to the output vector, along with its plane, column, and row
        """
        from_square = move.from_square
        to_square = move.to_square

        plane_index: int = None
        piece = self.cur_board.piece_at(from_square)
        direction = None

        if piece is None:
            raise Exception(f"No piece at {from_square}")

        if move.promotion and move.promotion != chess.QUEEN:
            piece_type, direction = Mapping.get_underpromotion_move(
                move.promotion, from_square, to_square)
            plane_index = Mapping.mapper[piece_type][1 - direction]
        else:
            # find the correct plane based on from_square and move_square
            if piece.piece_type == chess.KNIGHT:
                # get direction
                direction = Mapping.get_knight_move(from_square, to_square)
                plane_index = Mapping.mapper[direction]
            else:
                # get direction of queen-type move
                direction, distance = Mapping.get_queenlike_move(
                    from_square, to_square)
                plane_index = Mapping.mapper[direction][np.abs(distance)-1]
        # create a mask with only valid moves
        row = from_square % 8
        col = 7 - (from_square // 8)
        self.outputs.append((move, plane_index, row, col))
    
    def probabilities_to_actions(self, probabilities:torch.Tensor, board: str) -> dict:
        """
        Map the output vector of 4672 probabilities to moves. Returns a dictionary of moves and their probabilities.

        The output vector is a list of probabilities for every move
        * 4672 probabilities = 73*64 => 73 planes of 8x8

        The squares in these 8x8 planes indicate the square where the piece is.

        The plane itself indicates the type of move:
            - first 56 planes: queen moves (length of 7 squares * 8 directions)
            - next 8 planes: knight moves (8 directions)
            - final 9 planes: underpromotions (left diagonal, right diagonal, forward) * (three possible pieces (knight, bishop, rook))
        """
        probabilities = probabilities.reshape(
            config.amount_of_planes, config.n, config.n)
        # mask = np.zeros((config.amount_of_planes, config.n, config.n))

        actions = {}

        # only get valid moves
        self.cur_board = chess.Board(board)
        valid_moves = self.cur_board.generate_legal_moves()
        self.outputs = []
        
        # use threading to map valid moves quicker
        threads = []
        while True:
            try:
                move = next(valid_moves)
            except StopIteration:
                break
            thread = threading.Thread(
                target=self.map_valid_move, args=(move,))
            threads.append(thread)
            thread.start()

        # wait until all threads are done
        for thread in threads:
            thread.join()

        for move, plane_index, col, row in self.outputs:
            # mask[plane_index][col][row] = 1
            actions[move.uci()] = probabilities[plane_index][col][row]

        return actions
        
    def _select(self, node):
        "Find an unexplored descendent of `node`"
        
        root = node
        path = []

        while True:
            path.append(node)
            if node not in self.children or not self.children[node]:
                # node is either unexplored or terminal
                return path
            
            node = self._uct_select(node, root)  # descend a layer deeper
                    

    def _expand_simulate(self, node):
        "Update the `children` dict with the children of `node`"
        
        if node in self.children:
            return # leaf already explored
        
        board = node.board

        possible_actions = list(board.generate_legal_moves())

        if not len(possible_actions):
            assert board.is_game_over(), "Game is not over, but there are no possible moves?"
            outcome = board.outcome(claim_draw=True)
            if outcome is None:
                reward = 0
            else:
                reward = 1 if outcome.winner == self.agent.player else -1 # if the agent using this tree wins then we reward 1 else -1 
            print(f"Leaf's game ended with {self.Q[node]}")
            return reward

        # predict p and v
        # p = array of probabilities: [0, 1] for every move (including invalid moves)
        # v = [-1, 1]
        input_state = ChessEnv.state_to_input(board.fen())
        p, v = self.agent.predict(input_state)
        # map probabilities to moves, this also filters out invalid moves
        # returns a dictionary of moves and their probabilities
        # credis to the original author for ChessEnv and probabilties_to_actions and map_valid_move functions:https://github.com/zjeffer/chess-deep-rl/tree/main
        actions = self.probabilities_to_actions(p, board.fen())

        reward = v[0].item()
        children = set()
        # create a child node for every action
        for action in possible_actions:
            # make the move and get the new board
            child = node.make_move(board.san(action))
            # add a new child node with the new board, the action taken and its prior probability
            children.add(child) 
            self.P[child] = actions[action.uci()]
        self.children[node] = children
        
        return reward
    
    def _backpropagate(self, path, reward):
        "all the ancestors of leaf recieve the reward"

        for node in reversed(path):
            self.N[node] += 1
            self.Q[node] += reward
    
    def _uct_select(self, node, root):
        "select a child of node balancing exploitation and exploration"

        #  all children of node should already be expanded

        noise = [1 for _ in range(len(self.children[node]))]

        if self.stochastic and node == root:
            noise = np.random.dirichlet([config.DIRICHLET_NOISE]*len(self.children[node]))

        best_child = None
        best_score = -np.inf

        for i, child in enumerate(self.children[node]):
            ucb = self.exploration_weight * (self.P[child] * noise[i]) * (math.sqrt(self.N[node]) / (1 + self.N[child]))
            Q = self.Q[child]/(self.N[child] + 1)
            if node.board.turn == self.player:
                if best_score < Q + ucb:
                    best_score = Q + ucb
                    best_child = child
            else:
                if best_score < -Q + ucb:
                    best_score = -Q + ucb
                    best_child = child
        
        return best_child